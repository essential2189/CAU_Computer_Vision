{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN clean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lDYSzStAIisZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b8cbd3-b379-451d-ad02-f472f4399e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 24.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6JOxqKyGInLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ae82a0-9599-441a-8b17-b87e5536e6de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "seed_everything(seed=42)"
      ],
      "metadata": {
        "id": "6rA__qyTsBgM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import isdir\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from torch import tensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "DATASETS_PATH = Path(\"./datasets\")\n",
        "IMAGENET_MEAN = tensor([.485, .456, .406])\n",
        "IMAGENET_STD = tensor([.229, .224, .225])\n",
        "\n",
        "\n",
        "class LoadDataset:\n",
        "    def __init__(self, cls: str, size: int):\n",
        "        self.cls = cls\n",
        "        self.size = size\n",
        "        print('size:', size)\n",
        "        self.train_ds = TrainDataset(cls, size)\n",
        "        self.test_ds = TestDataset(cls, size)\n",
        "\n",
        "    def get_datasets(self):\n",
        "        return self.train_ds, self.test_ds\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        return DataLoader(self.train_ds), DataLoader(self.test_ds)\n",
        "\n",
        "\n",
        "class TrainDataset(ImageFolder):\n",
        "    def __init__(self, cls: str, size: int):\n",
        "        super().__init__(\n",
        "            root=DATASETS_PATH / cls / \"train\",\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize((size, size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                # transforms.CenterCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        self.cls = cls\n",
        "        self.size = size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, _ = self.samples[index]\n",
        "        sample = self.loader(path)\n",
        "\n",
        "        if \"good\" in path:\n",
        "            sample_class = 0\n",
        "        else:\n",
        "            sample_class = 1\n",
        "\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, sample_class\n",
        "\n",
        "\n",
        "class TestDataset(ImageFolder):\n",
        "    def __init__(self, cls: str, size: int):\n",
        "        super().__init__(\n",
        "            root=DATASETS_PATH / cls / \"test\",\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize((size, size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "                # transforms.CenterCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "            ]),\n",
        "        )\n",
        "        self.cls = cls\n",
        "        self.size = size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, _ = self.samples[index]\n",
        "        sample = self.loader(path)\n",
        "\n",
        "        if \"good\" in path:\n",
        "            sample_class = 0\n",
        "        else:\n",
        "            sample_class = 1\n",
        "\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, sample_class"
      ],
      "metadata": {
        "id": "RnuxoxPd3oHm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def knn_cluster(result_list, label_list, normal_list, anomaly_list):\n",
        "    scaler1 = StandardScaler()\n",
        "    pca1 = PCA(n_components=500)\n",
        "\n",
        "    train_scaler = scaler1.fit_transform(result_list)\n",
        "    train_reduce = pca1.fit_transform(train_scaler)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=2, n_jobs=-1)\n",
        "\n",
        "    knn.fit(train_reduce, label_list)\n",
        "\n",
        "    y_all = knn.predict(train_reduce).tolist()\n",
        "\n",
        "    return roc_auc_score(label_list, y_all)"
      ],
      "metadata": {
        "id": "rqRt5gl63j95"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import tensor\n",
        "import timm\n",
        "\n",
        "# import os\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'\n",
        "\n",
        "\n",
        "class train_feature_extractor(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(train_feature_extractor, self).__init__()\n",
        "\n",
        "        self.model = timm.create_model(\"wide_resnet50_2\", pretrained=True)\n",
        "\n",
        "        self.train_list = []\n",
        "        self.test_list = []\n",
        "\n",
        "        self.correct = 0\n",
        "        self.total = 0\n",
        "        self.result_list = []\n",
        "        self.label_list = []\n",
        "        self.anomaly_list = []\n",
        "        self.normal_list = []\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def __call__(self, x: tensor):\n",
        "        feature_maps = self.model(x.to(self.device))\n",
        "        return feature_maps\n",
        "\n",
        "    def train(self, train_dl):\n",
        "        for epoch in range(15):\n",
        "            i = 0\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in tqdm(train_dl):\n",
        "                optimizer.zero_grad()\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self(inputs)\n",
        "\n",
        "                loss = criterion(outputs, labels.to(torch.float32))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "                if i % 100 == 99:\n",
        "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "                    running_loss = 0.0\n",
        "                i += 1\n",
        "\n",
        "        print('Finished Training')\n",
        "\n",
        "\n",
        "def test_feature_extractor(net, test_dl):\n",
        "    result_list = []\n",
        "    label_list = []\n",
        "    anomaly_list = []\n",
        "    normal_list = []\n",
        "    with torch.no_grad():\n",
        "        for sample, label in tqdm(test_dl):\n",
        "            outputs = net(sample)\n",
        "\n",
        "            label_list.append(label[0])\n",
        "            result_list.append(outputs[0].tolist())\n",
        "            if label[0] == 0:\n",
        "                normal_list.append(outputs[0].tolist())\n",
        "            elif label[0] == 1:\n",
        "                anomaly_list.append(outputs[0].tolist())\n",
        "\n",
        "    return result_list, label_list, normal_list, anomaly_list"
      ],
      "metadata": {
        "id": "uHP2yTdS3rg4"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import click\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "ALLOWED_METHODS = [\"FPC\"]\n",
        "\n",
        "model = train_feature_extractor()\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    seed_everything(42)\n",
        "\n",
        "\n",
        "def loss_():\n",
        "    criterion = nn.MSELoss()\n",
        "    return criterion\n",
        "\n",
        "\n",
        "def optim_(model):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    return optimizer\n",
        "\n",
        "optimizer = optim_(model)\n",
        "criterion = loss_()\n",
        "\n",
        "def run_model(data_path: str):\n",
        "    train_ds, test_ds = LoadDataset(data_path, size=224).get_dataloaders()\n",
        "\n",
        "    MODEL_PATH = '/content/drive/MyDrive/trained_model/covid_wideResnet50.pth'\n",
        "\n",
        "    # model.train(train_ds)\n",
        "\n",
        "    # torch.save(model.state_dict(), MODEL_PATH)\n",
        "\n",
        "    pre_net = train_feature_extractor()\n",
        "    pre_net.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "    result_list, label_list, normal_list, anomaly_list = test_feature_extractor(pre_net, test_ds)\n",
        "\n",
        "    rocauc_score = knn_cluster(result_list, label_list, normal_list, anomaly_list)\n",
        "\n",
        "    print(rocauc_score)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "\n",
        "    # dataset = \"/content/drive/MyDrive/mymodel/datasets/Br35H/\"\n",
        "    dataset = \"/content/drive/MyDrive/mymodel/datasets/SARS-COV-2_Ct-Scan/\"\n",
        "    run_model(dataset)\n",
        "\n",
        "    print(\"run time :\", time.time() - start, \"sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2d_O2VM3uad",
        "outputId": "7c16d8b2-5571-4586-9179-ecd71b7f550d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size: 224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1583/1583 [00:37<00:00, 41.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.898961661341853\n",
            "run time : 41.67373275756836 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# brain50\n",
        "# 0.9663333333333333\n",
        "# run time : 955.2764930725098\n",
        "# pre run time : 42.81132102012634 sec\n",
        "\n",
        "\n",
        "# covid50\n",
        "# 0.9053514376996805\n",
        "# run time : 1234.8870503902435 sec\n",
        "# pre run time :  40.69177317619324 sec"
      ],
      "metadata": {
        "id": "fQztviaD47F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# brain101\n",
        "# 0.9176666666666666\n",
        "# run time : 1655.9852950572968 sec\n",
        "# pre run time : 67.74808979034424 sec\n",
        "\n",
        "# covid101\n",
        "# 0.90814696485623\n",
        "# run time : 1288.0728118419647 sec\n",
        "# pre run time :  57 sec"
      ],
      "metadata": {
        "id": "lVK6Z8tyuIch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# brain wide50\n",
        "# 0.996\n",
        "# run time : 1359.0688104629517 sec\n",
        "# pre run time : 50.63219332695007 sec\n",
        "\n",
        "\n",
        "# covid wide50\n",
        "# 0.898961661341853\n",
        "# run time : 1115.7649035453796 sec\n",
        "# pre run time : 41.67373275756836 sec"
      ],
      "metadata": {
        "id": "WymmraNHEofO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# brain\n",
        "# train - good 1126\n",
        "# test - good 374, anomaly 1500\n",
        "\n",
        "\n",
        "# covid\n",
        "# train - good 898\n",
        "# test - good 331, anomaly 1252"
      ],
      "metadata": {
        "id": "BhCzuS5n22tV"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}